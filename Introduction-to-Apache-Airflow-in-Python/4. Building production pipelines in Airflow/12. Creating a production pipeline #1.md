# Creating a production pipeline #1

Ide Exercise Exercise
Creating a production pipeline #1
You've learned a lot about how Airflow works - now it's time to implement your workflow into a production pipeline consisting of many objects including sensors and operators. Your boss is interested in seeing this workflow become automated and able to provide SLA reporting as it provides some extra leverage for closing a deal the sales staff is working on. The sales prospect has indicated that once they see updates in an automated fashion, they're willing to sign-up for the indicated data service.

From what you've learned about the process, you know that there is sales data that will be uploaded to the system. Once the data is uploaded, a new file should be created to kick off the full processing, but something isn't working correctly.

Refer to the source code of the DAG to determine if anything extra needs to be added.

Ide Exercise Instruction
100XP
Update the DAG in pipeline.py to import the needed operators.
Run the sense_file task from the command line and look for any errors. Use the command airflow tasks test <dag_id> <task_id> <date> and the appropriate arguments to run the command. For the last argument, use a -1 instead of a specific date.
Determine why the sense_file task does not complete and remedy this using the editor. Make sure to scroll through the terminal output to find any ERROR messages highlighted in red.
Re-test the sense_file task and verify the problem is fixed.

Take Hint (-30 XP)

Submit Answer

from airflow import DAG
from airflow.sensors.filesystem import FileSensor

# Import the needed operators
from airflow.operators.bash import ____
from airflow.operators.python import ____
from datetime import date, datetime

def process_data(**context):
  file = open('/home/repl/workspace/processed_data.tmp', 'w')
  file.write(f'Data processed on {date.today()}')
  file.close()

    
dag = DAG(dag_id='etl_update', default_args={'start_date': datetime(2023,4,1)})

sensor = FileSensor(task_id='sense_file', 
                    filepath='/home/repl/workspace/startprocess.txt',
                    poke_interval=5,
                    timeout=15,
                    dag=dag)

bash_task = BashOperator(task_id='cleanup_tempfiles', 
                         bash_command='rm -f /home/repl/*.tmp',
                         dag=dag)

python_task = PythonOperator(task_id='run_processing', 
                             python_callable=process_data,
                             dag=dag)

sensor >> bash_task >> python_task

Here's how to fix and complete the setup of your Airflow DAG:

1. **Import the Needed Operators**:
   - The DAG is missing imports for `BashOperator` and `PythonOperator`. Add these imports so the operators can be used.

2. **Corrected Code**:
   ```python
   from airflow import DAG
   from airflow.sensors.filesystem import FileSensor

   # Import the needed operators
   from airflow.operators.bash import BashOperator
   from airflow.operators.python import PythonOperator
   from datetime import date, datetime

   # Define the Python function for data processing
   def process_data(**context):
       file = open('/home/repl/workspace/processed_data.tmp', 'w')
       file.write(f'Data processed on {date.today()}')
       file.close()

   # Define the DAG
   dag = DAG(dag_id='etl_update', default_args={'start_date': datetime(2023, 4, 1)})

   # Define the FileSensor to monitor the file
   sensor = FileSensor(
       task_id='sense_file',
       filepath='/home/repl/workspace/startprocess.txt',
       poke_interval=5,
       timeout=15,
       dag=dag
   )

   # Define the BashOperator for cleaning up temp files
   bash_task = BashOperator(
       task_id='cleanup_tempfiles',
       bash_command='rm -f /home/repl/*.tmp',
       dag=dag
   )

   # Define the PythonOperator for processing data
   python_task = PythonOperator(
       task_id='run_processing',
       python_callable=process_data,
       dag=dag
   )

   # Set up task dependencies
   sensor >> bash_task >> python_task
   ```

3. **Command to Run `sense_file` Task**:
   Use the following command to test the `sense_file` task:
   ```bash
   airflow tasks test etl_update sense_file -1
   ```

### Troubleshooting Steps:
1. **Check for Missing Imports**: Ensure the `BashOperator` and `PythonOperator` are imported. If not, you may get an error indicating these classes are undefined.
2. **Examine the Output for Errors**:
   - Scroll through the terminal output to check for any **ERROR** messages highlighted in red.
   - Common issues include incorrect file paths, missing files, or sensor timeout errors.

3. **Common Issue: File Not Found**:
   - If the `sense_file` task does not complete, it's likely because the `startprocess.txt` file does not exist at the specified location (`/home/repl/workspace/startprocess.txt`). Make sure the file is present, or update the file path to the correct location.

4. **Re-Test**:
   - After making sure everything is correctly set up and the file exists, re-run the `sense_file` task using the `airflow tasks test` command to verify the issue is resolved.

This should set up your DAG properly and ensure that the `FileSensor` works as expected, kicking off the workflow once the file is detected.


You can cause the task to run via airflow tasks test etl_update sense_file -1. Make sure to put this command in the shell section of the workspace. Note that you will need to run this command, then fix the sense_file task as described below, then run this command a second time.Airflow Test Example
The sense_file task is an Airflow sensor. Look at the code in the DAG and try to create an appropriately named file. You can create this file in multiple ways, such as touch /home/repl/workspace/startprocess.txt via the command shell or right click and select New File in the menu on the left of the editor to create startprocess.txt.