# Creating a production pipeline #2

Creating a production pipeline #2
Continuing on your last workflow, you'd like to add some additional functionality, specifically adding some SLAs to the code and modifying the sensor components.

Refer to the source code of the DAG to determine if anything extra needs to be added. The default_args dictionary has been defined for you, though it may require further modification.

Ide Exercise Instruction
100XP
Add a sla of 90 minutes to the DAG using timedelta() function.
Update the FileSensor object to check for files every 45 seconds using poke_interval parameter.
Modify the python_task to send Airflow variables to the callable. Note that the callable is configured to accept the variables using the provide_context argument.

```pipeline.py
from airflow import DAG
from airflow.sensors.filesystem import FileSensor
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from dags.process import process_data
from datetime import timedelta, datetime

# Update the default arguments and apply them to the DAG
default_args = {
  'start_date': datetime(2023,1,1),
  ____
}

dag = DAG(dag_id='etl_update', default_args=default_args)

sensor = FileSensor(task_id='sense_file', 
                    filepath='/home/repl/workspace/startprocess.txt',
                    ____,
                    dag=dag)

bash_task = BashOperator(task_id='cleanup_tempfiles', 
                         bash_command='rm -f /home/repl/*.tmp',
                         dag=dag)

python_task = PythonOperator(task_id='run_processing', 
                             python_callable=process_data,
                             ____
                             dag=dag)

sensor >> bash_task >> python_task
```

```process.py
from datetime import date

def process_data(**kwargs):

    file = open("/home/repl/workspace/processed_data-" + kwargs['ds'] + ".tmp", "w")
    
    file.write(f"Data processed on {date.today()}")
    
    file.close()
  

```

To implement the necessary modifications to your Airflow DAG, follow these steps:

1. **Add the SLA of 90 minutes to the DAG** using the `timedelta()` function.
2. **Update the `FileSensor`** to check for files every 45 seconds using the `poke_interval` parameter.
3. **Modify the `PythonOperator`** to pass Airflow variables to the `process_data` function, utilizing the `provide_context` argument.

Here is how your updated `pipeline.py` should look:

```python
from airflow import DAG
from airflow.sensors.filesystem import FileSensor
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from dags.process import process_data
from datetime import timedelta, datetime

# Update the default arguments and apply them to the DAG
default_args = {
    'start_date': datetime(2023, 1, 1),
    'sla': timedelta(minutes=90)  # Add SLA of 90 minutes
}

# Define the DAG
dag = DAG(dag_id='etl_update', default_args=default_args)

# Update the FileSensor to check for files every 45 seconds
sensor = FileSensor(
    task_id='sense_file',
    filepath='/home/repl/workspace/startprocess.txt',
    poke_interval=45,  # Check every 45 seconds
    dag=dag
)

# Define a BashOperator for cleaning up temporary files
bash_task = BashOperator(
    task_id='cleanup_tempfiles',
    bash_command='rm -f /home/repl/*.tmp',
    dag=dag
)

# Modify the PythonOperator to pass Airflow variables using provide_context
python_task = PythonOperator(
    task_id='run_processing',
    python_callable=process_data,
    provide_context=True,  # Pass context variables
    dag=dag
)

# Set the task dependencies
sensor >> bash_task >> python_task
```

### Explanation of Changes
1. **Added SLA to the DAG**:
   - Used `timedelta(minutes=90)` to set an SLA of 90 minutes in the `default_args`.

2. **Modified `FileSensor`**:
   - Set `poke_interval=45` to ensure that the sensor checks for the file every 45 seconds.

3. **Updated `PythonOperator`**:
   - Included `provide_context=True` to pass the execution context variables (like `ds`) to the `process_data` function. This allows the function to access Airflow's built-in context variables.

With these updates, your DAG will now include an SLA, a properly configured file sensor, and context-aware execution for the Python task. Let me know if you need further assistance!