# Writing docstrings

Writing docstrings
We just learned some about the benefits of docstrings. In this exercise, you will practice writing docstrings that can be utilized by a documentation generator like Sphinx.

Note that your docstring submission must match the solution exactly. If you find yourself getting it wrong several times, it may be a good idea to refresh the sample code and start over.

Instructions
100 XP
Complete the portions of the docstring that document the parameters.
Complete the portion of the docstring describing the return value.
Complete the example function usage in the docstring.

# Complete the function's docstring
def tokenize(text, regex=r'[a-zA-z]+'):
  """Split text into tokens using a regular expression

  :____ text: text to be tokenized
  :param ____: regular expression used to match tokens using re.findall 
  :____: a list of resulting tokens

  >>> ____('the rain in spain')
  ____
  """
  return re.findall(regex, text, flags=re.IGNORECASE)

# Print the docstring
help(tokenize)


I'll help complete the docstring for the tokenize function. Let me first run the example to ensure we have the correct output.

```python
Execute
Copy Code
import re

def tokenize(text, regex=r'[a-zA-z]+'):
  """Split text into tokens using a regular expression
  
  :param text: text to be tokenized
  :param regex: regular expression used to match tokens using re.findall
  :return: a list of resulting tokens
  
  >>> tokenize('the rain in spain')
  ['the', 'rain', 'in', 'spain']
  """
  return re.findall(regex, text, flags=re.IGNORECASE)
```
# Print the docstring
help(tokenize)
The completed docstring fills in the blanks with:

:param text: for documenting the text parameter
regex for the regex parameter name
:return: for documenting the return value
tokenize for the function call in the example
['the', 'rain', 'in', 'spain'] for the expected output
When you run help(tokenize), it will display:

Help on function tokenize in module __main__:
```py
tokenize(text, regex='[a-zA-z]+')
    Split text into tokens using a regular expression
    
    :param text: text to be tokenized
    :param regex: regular expression used to match tokens using re.findall
    :return: a list of resulting tokens
    
    >>> tokenize('the rain in spain')
    ['the', 'rain', 'in', 'spain']
```
This docstring follows the standard format and includes:

A brief description of what the function does
Parameter documentation using :param:
Return value documentation using :return:
A doctest example showing usage and expected output