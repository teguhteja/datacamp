# Running an ETL Pipeline

Running an ETL Pipeline
Ready to run your first ETL pipeline? Let's get to it!

Here, the functions extract(), transform(), and load() have been defined for you. To run this data ETL pipeline, you're going to execute each of these functions. If you're curious, take a peek at what the extract() function looks like.

def extract(file_name):
    print(f"Extracting data from {file_name}")
    return pd.read_csv(file_name)
Instructions
100 XP
Use the extract() function to extract data from the raw_data.csv file.
Transform the extracted_data DataFrame using the transform() function.
Finally, load the transformed_data DataFrame to the cleaned_data SQL table.

# Extract data from the raw_data.csv file
extracted_data = ____(file_name="raw_data.csv")

# Transform the extracted_data
transformed_data = transform(data_frame=____)

# Load the transformed_data to cleaned_data.csv
____(data_frame=transformed_data, target_table="cleaned_data")

Here's how you can fill in the code to run the ETL pipeline:

```python
# Extract data from the raw_data.csv file
extracted_data = extract(file_name="raw_data.csv")

# Transform the extracted_data
transformed_data = transform(data_frame=extracted_data)

# Load the transformed_data to cleaned_data table
load(data_frame=transformed_data, target_table="cleaned_data")
```

### Explanation:
1. **Extract**: You call the `extract` function with `file_name="raw_data.csv"` to read the data from the CSV file.
2. **Transform**: You pass the `extracted_data` to the `transform` function to apply the transformation logic.
3. **Load**: Finally, you load the `transformed_data` to the "cleaned_data" table using the `load` function.

This code sequence runs the ETL pipeline by executing the `extract`, `transform`, and `load` functions in order.