# Data pipeline architecture patterns

Data pipeline architecture patterns
When building data pipelines, it's best to separate the files where functions are being defined from where they are being run.

In this exercise, you'll practice importing components of a pipeline into memory before using these functions to run the pipeline end-to-end. The project takes the following format, where pipeline_utils stores the extract(), transform(), and load() functions that will be used run the pipeline.

> ls
 etl_pipeline.py
 pipeline_utils.py
Instructions
100 XP
Import the extract, transform, and load functions from the pipeline_utils module.
Use the functions imported to run the data pipeline end-to-end.

# Import the extract, transform, and load functions from pipeline_utils
____

# Run the pipeline end to end by extracting, transforming and loading the data
raw_tax_data = ____("raw_tax_data.csv")
clean_tax_data = ____(raw_tax_data)
____(clean_tax_data, "clean_tax_data.parquet")

Here's how you can import the functions and run the data pipeline end-to-end:

```python
# Import the extract, transform, and load functions from pipeline_utils
from pipeline_utils import extract, transform, load

# Run the pipeline end to end by extracting, transforming, and loading the data
raw_tax_data = extract("raw_tax_data.csv")
clean_tax_data = transform(raw_tax_data)
load(clean_tax_data, "clean_tax_data.parquet")
```

### Explanation:
1. **Importing Functions**: Use `from pipeline_utils import extract, transform, load` to import the necessary functions from `pipeline_utils.py`.
2. **Running the Pipeline**: 
   - `extract()` loads the raw data.
   - `transform()` processes the raw data.
   - `load()` saves the transformed data to a Parquet file.