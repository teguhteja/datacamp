# Add batch normalization to your network

Add batch normalization to your network
Batch normalization is another form of regularization that rescales the outputs of a layer to make sure that they have mean 0 and standard deviation 1. In this exercise, we will add batch normalization to the convolutional neural network that we have used in previous exercises:

Convolution (15 units, kernel size 2, 'relu' activation)
Batch normalization
Convolution (5 unites, kernel size 2, 'relu' activation)
Flatten
Dense (3 units, 'softmax' activation)
A Sequential model along with Dense, Conv2D, Flatten, and Dropout objects are available in your workspace.

Instructions
100 XP
Add the first convolutional layer. You can use the img_rows and img_cols objects available in your workspace to define the input_shape of this layer.
Add batch normalization applied to the outputs of the first layer.

# Add a convolutional layer
____


# Add batch normalization layer
____

# Add another convolutional layer
model.add(Conv2D(5, kernel_size=2, activation='relu'))

# Flatten and feed to output layer
model.add(Flatten())
model.add(Dense(3, activation='softmax'))

To add batch normalization to your convolutional neural network in Keras, you will insert the `BatchNormalization` layer after the first convolutional layer. Below is the complete code with the appropriate sections filled in:

```python
# Add a convolutional layer
model.add(Conv2D(15, kernel_size=2, activation='relu', 
                 input_shape=(img_rows, img_cols, 1)))

# Add batch normalization layer
model.add(BatchNormalization())

# Add another convolutional layer
model.add(Conv2D(5, kernel_size=2, activation='relu'))

# Flatten and feed to output layer
model.add(Flatten())
model.add(Dense(3, activation='softmax'))
```

### Explanation of the Code:

1. **Convolutional Layer**:
   - **`model.add(Conv2D(15, kernel_size=2, activation='relu', input_shape=(img_rows, img_cols, 1)))`**: This line adds a convolutional layer with 15 filters, a kernel size of 2, and ReLU activation. It expects input images of shape `(img_rows, img_cols, 1)`.

2. **Batch Normalization Layer**:
   - **`model.add(BatchNormalization())`**: This line adds a batch normalization layer after the first convolutional layer. Batch normalization normalizes the outputs of the convolutional layer for each mini-batch, which helps stabilize and accelerate the training process. It rescales the outputs to have a mean of 0 and a standard deviation of 1, mitigating issues like internal covariate shift.

3. **Second Convolutional Layer**:
   - **`model.add(Conv2D(5, kernel_size=2, activation='relu'))`**: This line adds another convolutional layer with 5 filters and the same kernel size and activation function.

4. **Flatten Layer**:
   - **`model.add(Flatten())`**: This line flattens the output of the previous layer to prepare it for the fully connected (Dense) layer.

5. **Dense Layer**:
   - **`model.add(Dense(3, activation='softmax'))`**: Finally, this line adds a dense layer with 3 units and a softmax activation function, suitable for multi-class classification.

### Conclusion:
Adding batch normalization can enhance the training speed and model performance by ensuring that the inputs to each layer remain normalized. This can be particularly beneficial in deep networks where the accumulation of errors can lead to significant training difficulties.