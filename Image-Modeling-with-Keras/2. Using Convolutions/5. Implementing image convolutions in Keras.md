# Implementing image convolutions in Keras

1. Implementing convolutions in Keras
00:00 - 00:05
Keras has objects to represent convolution layers.

2. Keras Convolution layer
00:05 - 01:26
Here, we have a two-dimensional convolution, that we can use to analyze images. It resembles the "Dense" layers, but instead of having every unit in the layer connected to every unit in the previous layer, these connect to the previous layer through a convolution kernel. This means that the output of each unit in this layer is a convolution of a kernel over the image input. Here, we have ten convolution units. During training of a neural network that has convolutional layers, the kernels in each unit would be adjusted using back-propagation. The principle is the same as learning in the Dense layers that we have seen so far, but with fewer weights. A dense layer has one weight for each pixel in the image, but a convolution layer has only one weight for each pixel in the kernel. For example, if we set the kernel_size argument to 3, that means that the kernel of each unit has 9 pixels. If the layer has 10 units, it would have 90 parameters for these kernels.

3. Integrating convolution layers into a network
01:26 - 03:01
To build a network that contains a convolution layer, we need to import the Sequential model, and we will need the Dense as well as the Conv2D layers. In addition to these layers, we will also need a Flatten layer. This serves as a connector between convolution and densely connected layers. We initialize a Sequential model and add a first convolution layer. In addition to the function arguments that I showed you before, the number of units, the kernel size and the activation function, here we use the "relu" activation, we also add here the input shape. This is the size of each of the input images to the network. For example, for the images of clothing that we have been using, we might have image_rows and images_cols both equal to 28. To connect this layer to the next one, we add a Flatten layer. This takes the output of the convolution, that we previously referred to as a "feature map", and flattens it into a one-dimensional array. This is the expected input into the densely connected layer that is then added to the network as an output layer. Here, the output is one of three classes of clothing, so there are three units. To classify among the categories represented by the three units, we use the softmax activation function.

4. Our CNN
03:01 - 03:22
This is a diagram that describes the network that we created. The inputs are images of 28 by 28 pixels. The Conv2D operation goes to 10 feature maps of 28-by-28 pixels each and the Flatten operation takes us to 3 units of the output.

5. Fitting a CNN
03:22 - 04:32
Just like before, the next step is to compile the model, choosing the optimizer, and the loss function that we would like to use for fitting. For classification tasks categorical cross-entropy is an appropriate loss function. We can also specify the metrics we would like to see during training, here just the accuracy. Our training data are 50 samples of images of clothing. Each training item is a 28 by 28 pixel image with one channel. When we trained a fully-connected network, composed entirely of densely connected layers, we had to reshape this input, before feeding it to the network. Here, we would like the pixels to retain their spatial relationships, so we don't do that. That is why we had to specify the input shape when we defined the convolutional layer. We fit the model with training data and training labels, after which we can test the accuracy of the model on a separate test data set.

6. Let's practice!
04:32 - 04:38
Time to put this into practice.