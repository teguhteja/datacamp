# Evaluating a classifier

Evaluating a classifier
To evaluate a classifier, we need to test it on images that were not used during training. This is called "cross-validation": a prediction of the class (e.g., t-shirt, dress or shoe) is made from each of the test images, and these predictions are compared with the true labels of these images.

The results of cross-validation are provided as one-hot encoded arrays: test_labels and predictions.

Instructions
100 XP
Multiply the arrays with each other and sum the result to find the total number of correct predictions.
Divide the number of correct answers (the sum) by the length of predictions array to calculate the proportion of correct predictions.


# Calculate the number of correct predictions
number_correct = ____
print(number_correct)

# Calculate the proportion of correct predictions
proportion_correct = ____
print(proportion_correct)

To evaluate a classifier using one-hot encoded arrays, we want to find out how many predictions match the true labels, then calculate the accuracy as the proportion of correct predictions. Here's how you can accomplish this:

### Steps:

1. **Multiply** the test labels and predictions. In one-hot encoding, for each correct prediction, only one element will be `1` at the same position in both arrays.
2. **Sum** the result to find how many correct predictions were made.
3. **Divide** the number of correct predictions by the total number of predictions to get the proportion (accuracy).

Here's the code with the blanks filled:

```python
# Calculate the number of correct predictions
number_correct = (test_labels * predictions).sum()
print(number_correct)

# Calculate the proportion of correct predictions
proportion_correct = number_correct / len(predictions)
print(proportion_correct)
```

### Explanation:

- **`test_labels * predictions`**: This multiplies the one-hot encoded arrays element-wise. A `1` in the same position for both arrays indicates a correct prediction.
- **`.sum()`**: This sums up the correct predictions, as all the correct ones will have a `1`, and incorrect predictions will result in `0`.
- **`len(predictions)`**: This gives the total number of predictions, used to calculate the accuracy (proportion of correct predictions).

For example, if you had 5 predictions, and 4 of them were correct, the output would show:

```bash
4
0.8
```

This means the classifier got 4 out of 5 predictions correct, resulting in an 80% accuracy.