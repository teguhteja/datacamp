# Treating duplicates

Treating duplicates
In the last exercise, you were able to verify that the new update feeding into ride_sharing contains a bug generating both complete and incomplete duplicated rows for some values of the ride_id column, with occasional discrepant values for the user_birth_year and duration columns.

In this exercise, you will be treating those duplicated rows by first dropping complete duplicates, and then merging the incomplete duplicate rows into one while keeping the average duration, and the minimum user_birth_year for each set of incomplete duplicate rows.

Instructions
100 XP
Drop complete duplicates in ride_sharing and store the results in ride_dup.
Create the statistics dictionary which holds minimum aggregation for user_birth_year and mean aggregation for duration.
Drop incomplete duplicates by grouping by ride_id and applying the aggregation in statistics.
Find duplicates again and run the assert statement to verify de-duplication.

# Drop complete duplicates from ride_sharing
ride_dup = ____.____()

# Create statistics dictionary for aggregation function
statistics = {'user_birth_year': ____, 'duration': ____}

# Group by ride_id and compute new statistics
ride_unique = ride_dup.____('____').____(____).reset_index()

# Find duplicated values again
duplicates = ride_unique.____(subset = 'ride_id', keep = False)
duplicated_rides = ride_unique[duplicates == True]

# Assert duplicates are processed
assert duplicated_rides.shape[0] == 0


Here's how to treat duplicates in the `ride_sharing` DataFrame by dropping complete duplicates and merging incomplete duplicate rows:

```python
# Drop complete duplicates from ride_sharing
ride_dup = ride_sharing.drop_duplicates()

# Create statistics dictionary for aggregation function
statistics = {'user_birth_year': 'min', 'duration': 'mean'}

# Group by ride_id and compute new statistics
ride_unique = ride_dup.groupby('ride_id').agg(statistics).reset_index()

# Find duplicated values again
duplicates = ride_unique.duplicated(subset='ride_id', keep=False)
duplicated_rides = ride_unique[duplicates == True]

# Assert duplicates are processed
assert duplicated_rides.shape[0] == 0
```

### Explanation:
1. **Drop Complete Duplicates**:
   - `ride_sharing.drop_duplicates()` removes all rows that are exactly the same (complete duplicates) from the DataFrame, storing the result in `ride_dup`.

2. **Create Statistics Dictionary**:
   - The `statistics` dictionary defines how to aggregate the `user_birth_year` and `duration` columns:
     - For `user_birth_year`, we take the minimum value.
     - For `duration`, we take the mean value.

3. **Group and Aggregate**:
   - `ride_dup.groupby('ride_id').agg(statistics).reset_index()` groups the data by `ride_id`, applies the aggregation defined in the `statistics` dictionary, and resets the index to maintain a clean DataFrame.

4. **Find Duplicates Again**:
   - `duplicates = ride_unique.duplicated(subset='ride_id', keep=False)` checks for any remaining duplicates in the `ride_id` column.

5. **Assert Duplicates are Processed**:
   - `assert duplicated_rides.shape[0] == 0` ensures that there are no duplicates remaining in the processed DataFrame. If this condition is true, the assertion passes without error.